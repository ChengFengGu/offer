# 1. 什么是过拟合，有哪些解决方法

## 1.1. 过拟合

模型对训练数据拟合呈现过当，反映在指标上，就是模型在训练集上表现良好，但在测试集和新数据上的表现较差。

### 1.1.1. 如何解决

1. 增加数据量

可以让模型学习到更多，更有效的特征，减少噪声的影响。可以用数据增广的方式进行扩充。进一步的，可以使用生成对抗网络生成大量的新的训练数据。

2. 降低模型复杂度

在数据较少的时候，模型过于复杂是产生过拟合的主要因素，适当降低模型复杂度可以避免模型拟合过多的采样噪声。例如，在神经网络中减少网络层数，神经元个数，在决策树模型中降低树的深度、进行剪枝等。

3. 正则化方法

给模型的参数加上一定的正则化约束，比如将权值的大小加入到损失函数中。以L2正则化为例：

$C = C_0 + \frac{\lambda}{2n}\times \sum \limits_{i}\omega_i^2$

这样，在优化原来的目标函数$C_0$的同时，也能避免权值过大带来的过拟合风险

4. 集成学习的方法

集成学习方法是把多个模型集成在一起，来降低单一模型的过拟合风险，如Bagging方法（随机森林）和Boosting方法（AdaBoost)

Bagging + 决策树  = 随机森林
AdaBoost + 决策树 = 提升树
## 1.2. 欠拟合

模型在训练和预测时的表现都不好。

![](https://garden-lu-oss.oss-cn-beijing.aliyuncs.com/images20211026113445.png)





# 2. dropout 的原理

# 3. BatchNormalization 的原理 公式

# 4. L1和L2都是如何解决过拟合问题的


# 5. 交叉熵是什么？计算交叉熵？



# 6. 为什么要使用激活函数？ 常见的激活函数，各自的优缺点？



# 7. CNN给定图像和卷积核，求输出的形状

输入(3,32,32),10个卷积核，3*3 ， 步长为1


# 8. LSTM 原理

# 9. 常用的评估标准

- AUC
- ROC
- F1
- Recall
- Accuracy

# 10. 介绍项目


# 11. 决策树 随机森林 adaboost gbdt xgboost




# 12. L1正则化使得模型参数具有稀疏性的原理是什么？

![](https://garden-lu-oss.oss-cn-beijing.aliyuncs.com/images20211026143519.png)

直观上看，是因为L2的解空间