# 1. 什么是过拟合，有哪些解决方法

## 1.1. 过拟合

模型对训练数据拟合呈现过当，反映在指标上，就是模型在训练集上表现良好，但在测试集和新数据上的表现较差。

### 1.1.1. 如何解决

1. 增加数据量

可以让模型学习到更多，更有效的特征，减少噪声的影响。可以用数据增广的方式进行扩充。进一步的，可以使用生成对抗网络生成大量的新的训练数据。

2. 降低模型复杂度

在数据较少的时候，模型过于复杂是产生过拟合的主要因素，适当降低模型复杂度可以避免模型拟合过多的采样噪声。例如，在神经网络中减少网络层数，神经元个数，在决策树模型中降低树的深度、进行剪枝等。

3. 正则化方法

给模型的参数加上一定的正则化约束，比如将权值的大小加入到损失函数中。以L2正则化为例：

$C = C_0 + \frac{\lambda}{2n}\times \sum \limits_{i}\omega_i^2$

这样，在优化原来的目标函数$C_0$的同时，也能避免权值过大带来的过拟合风险

4. 集成学习的方法

集成学习方法是把多个模型集成在一起，来降低单一模型的过拟合风险，如Bagging方法（随机森林）和Boosting方法（AdaBoost)

Bagging + 决策树  = 随机森林
AdaBoost + 决策树 = 提升树
## 1.2. 欠拟合

模型在训练和预测时的表现都不好。

![](https://garden-lu-oss.oss-cn-beijing.aliyuncs.com/images20211026113445.png)


### 1.2.1. 如何解决

1. 添加新特征


当特征不足，或者特征与样本标签相关性不强的时候，模型容易出现过拟合。通过挖掘“上下文特征”、“ID类特征”、“组合特征”等新的特征，往往能够取得更好的效果。在深度学习潮流中，因子分解机，梯度提升决策树，Deep-Crossing等都可成为丰富特征的方法。

2. 增加模型复杂度

例如，在线性模型中添加高次项，在神经网络模型中增加网络层数或神经元个数。

3. 减少正则化项系数

减小惩罚力度


# 2. dropout 的原理


## 2.1. 思想
Dropout 是指在深度网络的训练中，以一定的概率随机地“临时丢弃”一部分神经元节点。具体来讲，Dropout作用于每个批次，由于其随机丢弃部分神经元的机制，相当于每次迭代都在训练不同结构的神经网络，暗合了集成学习的思想。

类比于同样属于集成学习的Bagging方法，Bagging方法需要同时训练和评估多个基模型，当模型参数量达到一定规模的时候，这种方式需要消耗大量的运算时间和空间。Dropout在批次级别上操作，提供了一种轻量级的Bagging近似，能够实现指数级数量的神经网络的训练与评测。

## 2.2. 细节
具体实现过程中，要求某个神经元激活值以一定的概率值$p$被丢弃，即该神经元暂时停止工作，

![](https://garden-lu-oss.oss-cn-beijing.aliyuncs.com/images20211026151934.png)

对于包含N个神经元节点的网络，在Dropout的作用下可以看作为$2^N$个模型的集成。它们可以看作是原始模型的子网络，共享部分权值，并且具有相同的网络层数，而模型整体的参数数目不变，大大简化了运算。

对于任意神经元，每次训练中都与一组随机挑选的不同的神经元集合共同进行优化，这个过程会减弱全体神经元之间的联合适应性，减少过拟合的风险，增强泛化能力。

## 2.3. 训练和预测时，Dropout的作用机理

在训练阶段，每个神经元节点需要增加一个概率系数，在前向传播时，原始的传播公式为：

$$
z_i^{l+1} = w_i^{l+1}y_i^{l} + b_i^{l+1}
$$

$$
y_i^{l+1} = f(z_i^{l+1})
$$


加入了Dropout之后，

$$
r_i^{l+1} \sim Bernoulli(p)
$$

$$
\widetilde{y_i}^l  = r_i \cdot y_i^l
$$

$$
z_i^{l+1} = w_i^{l+1} \widetilde{y_i}^l + b_i^{l+1}
$$

$$
y_i^{l+1} = f(z_i^{l+1})
$$


测试阶段每个神经元的参数直接乘以参数$p$


# 3. BatchNormalization 的原理 公式

神经网络训练的本质是学习数据分布，如果训练数据与测试数据分布不同将大大降低网络的泛化能力，因此我们需要在训练开始前对所有的输入数据进行归一化处理。


## 3.1. 解决的问题
然而，随着网络训练的进行，每个隐层的参数变化使得后一层的输入发生变化，从而每一批数据的分布也随之改变，致使网络在每一次迭代中都需要拟合不同的数据分布，增大训练的复杂度，过拟合的风险也随之提高。

## 3.2. 用的方法
批量归一化方法是针对每一批数据，在网络的每一层输入之前增加归一化处理（均值为0，标准差为1），将所有的批数据强制在统一的数据分布下，即对该层任意一个神经元（假设为第k维）$\hat{x}^{(k)}$采用的公式如下：

$$
\hat{x}^{(k)} = \frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}
$$

其中$x^{(k)}$为该层第k个神经元的原始输入数据，$E[x^{(k)}]$为这一批数据数据在第k个神经元的均值.


批量归一化可以看作是在每一层输入和上一层输出之间加入了一个新的计算层，对数据分布进行额外的约束，从而增强模型的泛化能力。但是批量归一化，同时也降低了模型的拟合能力，归一化之后的输入分布被强制为0均值和1标准差。

以Sigmoid为例，批量归一化之后数据整体处于函数的非饱和区域，只包含线性变换，破坏了之前学习到的特征分布。为了恢复原始数据分布，具体实现中引入了变换重构以及可学习参数$\beta$ 和 $\gamma$

$$
y^{(k)} = \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)} 
$$

其中γ(k)和β(k)分别为输入数据分布的方差和偏差。

对于一般的网络，不采用批量归 一化操作时，这两个参数高度依赖前面网络学习到的连接权重（对应复杂的非线 性）。而在批量归一化操作中，γ和β变成了该层的学习参数，仅用两个参数就可 以恢复最优的输入数据分布，与之前网络层的参数解耦，从而更加有利于优化的 过程，提高模型的泛化能力


## 3.3. 完整公式

完整的批量归一化网络层的前向传导过程公式如下：


对于批次B：

$$
\mu_B \leftarrow \frac{1}{m}\sum_{i=1}^mx_i
$$

$$
\sigma_B^2 \leftarrow \sum_{i=1}^m(x_i-\mu_B)^2
$$

$$
\hat{x_i} \leftarrow \frac{x_i  - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
$$

$$
y_i \leftarrow \gamma \hat{x_i} + \beta \equiv BN_{\gamma,\beta}(x_i)
$$

# 4. L1和L2都是如何解决过拟合问题的




# 5. 交叉熵是什么？计算交叉熵？



# 6. 为什么要使用激活函数？ 常见的激活函数，各自的优缺点？



# 7. CNN给定图像和卷积核，求输出的形状

输入(3,32,32),10个卷积核，3*3 ， 步长为1


# 8. LSTM 原理

# 9. 常用的评估标准

- AUC
- ROC
- F1
- Recall
- Accuracy

# 10. 介绍项目


# 11. 决策树 随机森林 adaboost gbdt xgboost




# 12. L1正则化使得模型参数具有稀疏性的原理是什么？

![](https://garden-lu-oss.oss-cn-beijing.aliyuncs.com/images20211026143519.png)

直观上看，是因为L2的解空间是圆形，更容易与等高线在非0处相撞， L1的解空间是方形，更容易在尖角上相撞。


- 为什么加入正则项就是定义了一个解空间的约束
- 为什么L1和L2的解空间是不同的？

从理论上推导，KKT条件

# 13. 什么是KKT条件

