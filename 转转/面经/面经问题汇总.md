# 1. 什么是过拟合，有哪些解决方法

## 1.1. 过拟合

模型对训练数据拟合呈现过当，反映在指标上，就是模型在训练集上表现良好，但在测试集和新数据上的表现较差。

### 1.1.1. 如何解决

1. 增加数据量

可以让模型学习到更多，更有效的特征，减少噪声的影响。可以用数据增广的方式进行扩充。进一步的，可以使用生成对抗网络生成大量的新的训练数据。

2. 降低模型复杂度

在数据较少的时候，模型过于复杂是产生过拟合的主要因素，适当降低模型复杂度可以避免模型拟合过多的采样噪声。例如，在神经网络中减少网络层数，神经元个数，在决策树模型中降低树的深度、进行剪枝等。

3. 正则化方法

给模型的参数加上一定的正则化约束，比如将权值的大小加入到损失函数中。以L2正则化为例：

$C = C_0 + \frac{\lambda}{2n}\times \sum \limits_{i}\omega_i^2$

这样，在优化原来的目标函数$C_0$的同时，也能避免权值过大带来的过拟合风险

4. 集成学习的方法

集成学习方法是把多个模型集成在一起，来降低单一模型的过拟合风险，如Bagging方法（随机森林）和Boosting方法（AdaBoost)

Bagging + 决策树  = 随机森林
AdaBoost + 决策树 = 提升树
## 1.2. 欠拟合

模型在训练和预测时的表现都不好。

![](https://garden-lu-oss.oss-cn-beijing.aliyuncs.com/images20211026113445.png)


### 1.2.1. 如何解决

1. 添加新特征


当特征不足，或者特征与样本标签相关性不强的时候，模型容易出现过拟合。通过挖掘“上下文特征”、“ID类特征”、“组合特征”等新的特征，往往能够取得更好的效果。在深度学习潮流中，因子分解机，梯度提升决策树，Deep-Crossing等都可成为丰富特征的方法。

2. 增加模型复杂度

例如，在线性模型中添加高次项，在神经网络模型中增加网络层数或神经元个数。

3. 减少正则化项系数

减小惩罚力度


# 2. dropout 的原理


## 2.1. 思想
Dropout 是指在深度网络的训练中，以一定的概率随机地“临时丢弃”一部分神经元节点。具体来讲，Dropout作用于每个批次，由于其随机丢弃部分神经元的机制，相当于每次迭代都在训练不同结构的神经网络，暗合了集成学习的思想。

类比于同样属于集成学习的Bagging方法，Bagging方法需要同时训练和评估多个基模型，当模型参数量达到一定规模的时候，这种方式需要消耗大量的运算时间和空间。Dropout在批次级别上操作，提供了一种轻量级的Bagging近似，能够实现指数级数量的神经网络的训练与评测。

## 2.2. 细节
具体实现过程中，要求某个神经元激活值以一定的概率值$p$被丢弃，即该神经元暂时停止工作，

![](https://garden-lu-oss.oss-cn-beijing.aliyuncs.com/images20211026151934.png)

对于包含N个神经元节点的网络，在Dropout的作用下可以看作为$2^N$个模型的集成。它们可以看作是原始模型的子网络，共享部分权值，并且具有相同的网络层数，而模型整体的参数数目不变，大大简化了运算。

对于任意神经元，每次训练中都与一组随机挑选的不同的神经元集合共同进行优化，这个过程会减弱全体神经元之间的联合适应性，减少过拟合的风险，增强泛化能力。

## 2.3. 训练和预测时，Dropout的作用机理

在训练阶段，每个神经元节点需要增加一个概率系数，在前向传播时，原始的传播公式为：

$$
z_i^{l+1} = w_i^{l+1}y_i^{l} + b_i^{l+1}
$$

$$
y_i^{l+1} = f(z_i^{l+1})
$$


加入了Dropout之后，

$$
r_i^{l+1} \sim Bernoulli(p)
$$

$$
\widetilde{y_i}^l  = r_i \cdot y_i^l
$$

$$
z_i^{l+1} = w_i^{l+1} \widetilde{y_i}^l + b_i^{l+1}
$$

$$
y_i^{l+1} = f(z_i^{l+1})
$$


测试阶段每个神经元的参数直接乘以参数$p$


# 3. BatchNormalization 的原理 公式

神经网络训练的本质是学习数据分布，如果训练数据与测试数据分布不同将大大降低网络的泛化能力，因此我们需要在训练开始前对所有的输入数据进行归一化处理。


## 3.1. 解决的问题
然而，随着网络训练的进行，每个隐层的参数变化使得后一层的输入发生变化，从而每一批数据的分布也随之改变，致使网络在每一次迭代中都需要拟合不同的数据分布，增大训练的复杂度，过拟合的风险也随之提高。

## 3.2. 用的方法
批量归一化方法是针对每一批数据，在网络的每一层输入之前增加归一化处理（均值为0，标准差为1），将所有的批数据强制在统一的数据分布下，即对该层任意一个神经元（假设为第k维）$\hat{x}^{(k)}$采用的公式如下：

$$
\hat{x}^{(k)} = \frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}
$$

其中$x^{(k)}$为该层第k个神经元的原始输入数据，$E[x^{(k)}]$为这一批数据数据在第k个神经元的均值.


批量归一化可以看作是在每一层输入和上一层输出之间加入了一个新的计算层，对数据分布进行额外的约束，从而增强模型的泛化能力。但是批量归一化，同时也降低了模型的拟合能力，归一化之后的输入分布被强制为0均值和1标准差。

以Sigmoid为例，批量归一化之后数据整体处于函数的非饱和区域，只包含线性变换，破坏了之前学习到的特征分布。为了恢复原始数据分布，具体实现中引入了变换重构以及可学习参数$\beta$ 和 $\gamma$

$$
y^{(k)} = \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)} 
$$

其中γ(k)和β(k)分别为输入数据分布的方差和偏差。

对于一般的网络，不采用批量归 一化操作时，这两个参数高度依赖前面网络学习到的连接权重（对应复杂的非线 性）。而在批量归一化操作中，γ和β变成了该层的学习参数，仅用两个参数就可 以恢复最优的输入数据分布，与之前网络层的参数解耦，从而更加有利于优化的 过程，提高模型的泛化能力


## 3.3. 完整公式

完整的批量归一化网络层的前向传导过程公式如下：


对于批次B：

$$
\mu_B \leftarrow \frac{1}{m}\sum_{i=1}^mx_i
$$

$$
\sigma_B^2 \leftarrow \sum_{i=1}^m(x_i-\mu_B)^2
$$

$$
\hat{x_i} \leftarrow \frac{x_i  - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
$$

$$
y_i \leftarrow \gamma \hat{x_i} + \beta \equiv BN_{\gamma,\beta}(x_i)
$$

# 4. L1和L2都是如何解决过拟合问题的

## 4.1. 降低模型复杂度
其实主要原理就是通过引入权重参数来限制模型的复杂度，从而提高模型的泛化能力。


L2正则项中的 λ 参数越大，模型中的权重矩阵的值就越小。模型中就会有很多隐藏单元的权重值非常小，这些隐藏单元在模型中的影响就变得很小，这就相当于把神经网络简化为一个较小的网络，但是网络深度不变，于是模型就会从过拟合状态向欠拟合状态转换。调整 λ 参数，可以使模型介于过拟合和欠拟合之间。

## 4.2. 降低大值

同时L2正则化可以对大数值权重进行惩罚，这使得没有哪个特征能够单独对整个模型有过大的影响，即每个维度对最终结果的影响都不是很大，使模型把大多数维度上的特征都利用起来，而不是只依赖其中少数几个特征。


L1正则化主要用于挑选出重要的特征，并舍弃不重要的特征。一定程度上降低了模型的复杂度。


L1正则化会趋向于选择稀疏矩阵，即只使用一部分特征，而其他的特征都为0；而L2正则化基本上会使用更多的特征，不过每个特征发挥的作用都比较小。在只有少部分特征起重要作用的情况下，可以选择L1范数，因为它会自动地选择重要的特征。如果大部分特征都对结果起作用，即便起的作用很小很平均，这时使用L2范数更合适。


# 5. 常见的损失函数

在有监督学习中， 损失函数刻画了模型和训练样本的匹配程度。


假设训练样本的形式为$(x_i,y_i)$，模型$f(x_i,\theta):X\rightarrow Y$

## 5.1. 0-1损失

非凸，非光滑，难以优化

## 5.2. 0-1函数的代理


- Logistic 

$$
L_{logistic}(f,y) = log_2(1+e^{-fy})
$$


- 交叉熵损失函数



# 6. 交叉熵是什么？计算交叉熵？



$-\sum \limits_{i=1}^N \sum \limits_{k=1}^K y_i^k log(p_i^k)$

- 信息量

$$
I(x) = -log(p(x))
$$

- 熵

$$
H(x) = -p(x)log(p(x))
$$

- 相对熵 KL散度 

$$
D_{KL}(p||q) = \sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)})
$$

描述两个分布之间的接近程度

- 交叉熵

将其变形

$$
D_{KL}(p||q) = \sum_{i=1}^np(x_i)log(p(x_i)) - \sum_{i=1}^np(x_i)log(q(x_i)) = -H(p(x)) + [-\sum_{i=1}^np(x_i)log(q(x_i))]
$$

相当于还是在评估模型输出和真实数据分布之间的差异,但是减去不变的部分,只关注交叉熵就够了.


# 7. 假设检验




# 8. 为什么要使用激活函数？ 常见的激活函数，各自的优缺点？


## 8.1. sigmoid

$$
f(x) =  \frac{1}{1 + e^{-x}}
$$


### 8.1.1. 优点

具有指数的形状，它在物理意义上最为接近神经元。sigmoid的输出是（0，1），具有很好的性质

### 8.1.2. 缺点

1. 梯度消失（饱和性）
2. 偏移现象

sigmoid函数的输出值均大于0，使得输出不是0的均值，这会导致后一层的神经元将得到上一层非0均值的信号作为输入。


## 8.2. tanh

$$
\tanh(x) = \frac{1-e^{-2x}}{1+e^{-2x}}
$$

tanh也是一种非常常见的激活函数，与sigmoid相比，它的输出均值为0，这使得它的收敛速度要比sigmoid快，减少了迭代更新的次数.

### 8.2.1. 优点

1. 相比sigmoid 收敛快 

因为均值为0

### 8.2.2. 缺点

1. 饱和性 梯度消失

# 9. 各个池化函数 和 各自的优缺点

# 10. CNN给定图像和卷积核，求输出的形状

输入(3,32,32),10个卷积核，3*3 ， 步长为1


# 11. LSTM 原理

# 12. 常用的评估标准

- AUC
- ROC
- F1
- Recall
- Accuracy

# 13. 介绍项目


# 14. 决策树 随机森林 adaboost gbdt xgboost


# 15. 归并排序 复杂度 稳定性


# 16. 一个骰子，投N次，最大概率的和是多少

# 17. 一个数组，求第二大的数

# 18. 具体讲什么叫attention机制，attention最开始是NLP提出吗，从图像角度解释为什么attention。

# 19. L1正则化使得模型参数具有稀疏性的原理是什么？

![](https://garden-lu-oss.oss-cn-beijing.aliyuncs.com/images20211026143519.png)

直观上看，是因为L2的解空间是圆形，更容易与等高线在非0处相撞， L1的解空间是方形，更容易在尖角上相撞。


- 为什么加入正则项就是定义了一个解空间的约束
- 为什么L1和L2的解空间是不同的？

从理论上推导，KKT条件



