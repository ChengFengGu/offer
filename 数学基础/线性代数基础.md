# 1. 基础概念

## 1.1. 最最基础概念

略 #TODO

## 1.2. 补充

### 1.2.1. 范数

在机器学习中，使用范数来衡量向量的大小


$$
\|x\|_{p}=\left(\sum_{i}\left|x_{i}\right|^{p}\right)^{\frac{1}{p}}
$$


常见：
1. 1范数
$$
\|\mathrm{X}\|_{1}=\sum_{i}\left|x_{i}\right|
$$

2. 2范数

$$
\|x\|_{2}=\sqrt{\sum_{i=1}^{n} x_{i}^{2}}
$$

3. 无穷范数

$$
p=\infty,\|x\|_{\infty}=\max _{1 \leq i \leq n}\left|x_{i}\right|_{0}
$$

范数是将向量映射到非负值的函数。直观来看，向量x的范数衡量从原点到点x的距离。Lp范数示例如图：

![](https://garden-lu-oss.oss-cn-beijing.aliyuncs.com/images20211019171353.png)

### 1.2.2. 张量

张量的概念是由G.Ricci在19世纪末提出来的。G.Ricci研究张量的目的是为几何性质和物理规律的表达寻求一种在坐标变换下形式保持不变。他所考虑的张量是如同向量分量的一个数组，要求张量在坐标变换下服从某种线性变换的规律。近代理论已经把张量叙述成向量空间V及其对偶空间V*上的多重线性函数，但是用分量表示张量仍有它的重要意义，尤其是涉及张量的计算时更是如此。 #TODO


$$
\sigma=\left[\begin{array}{lll}
\sigma_{11} & \sigma_{12} & \sigma_{13} \\
\sigma_{21} & \sigma_{22} & \sigma_{23} \\
\sigma_{31} & \sigma_{32} & \sigma_{33}
\end{array}\right]=\left[\begin{array}{lll}
\sigma_{x x} & \sigma_{x y} & \sigma_{x z} \\
\sigma_{y x} & \sigma_{y y} & \sigma_{y z} \\
\sigma_{z x} & \sigma_{z y} & \sigma_{z z}
\end{array}\right]
$$


![](https://garden-lu-oss.oss-cn-beijing.aliyuncs.com/images20211019171553.png)


## 1.3. 特征向量和特征值

$$
\boldsymbol{A} \boldsymbol{x} = \lambda \boldsymbol{x}
$$

标量λ被称为这个特征向量对应的特征值。类似地，我们也可以定义左特征向量xTA=λxT，但是通常更关注右特征向量。如果υ是A的特征向量，那么其缩放后也是A的特征向量。此外，鉴于sυ和υ有相同的特征值，因此我们通常只考虑单位特征向量。

假设矩阵A有n个线性无关的特征向量{υ(1)，…，υ(n)}，对应的特征值为{λ1，…，λn}，我们可将特征向量连接成一个矩阵使得每一列是一个特征向量[υ(1)，…，υ(n)]，同时也可以将特征值连接成一个向量λ=[λ1，…，λn]T，则可以将特征分解为：

$$
\boldsymbol{A}=\boldsymbol{V} \operatorname{diag}(\boldsymbol{\lambda}) \boldsymbol{V}^{-1}
$$

不是每个矩阵都能够分解成特征值和特征向量。在一些情况下，特征分解可能会涉及复数和非实数。每个实对称矩阵都可以分解成实特征向量和实特征值：

$$
\boldsymbol{A}=\boldsymbol{Q} \Lambda \boldsymbol{Q}^{-1}
$$

