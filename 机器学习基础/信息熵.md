# 1. 熵

对于$P(X) = P(X=x) (x \in R)$，其熵为：

$$
H(X) = -\sum_{x\in R}P(x)log_{2}P(x)
$$

其实它就类似于一个期望的公式，不同的是 ，它衡量的是信息的无序程度。 观察其单调性可以看出来，当一个事件越确定（概率很大，或很小），那么，它的熵就越高。反之，则越低。


例如，一个二元的信息熵可以表示为：


$$
H(X)=-p(x) \log _{2} p(x)-(1-p(x)) \log _{2} p(x)
$$

它的变化曲线如下：

![](https://garden-lu-oss.oss-cn-beijing.aliyuncs.com/images20211019203018.png)

> $H(X) \leq log_2|X|$  , 当且仅当概率平均分布的时候，$H(X)$的最大值为$P(X)=\frac{1}{|X|}$

# 2. 联合熵

如果（X，Y）表示一对离散随机变量的不确定性，即X，Y~p(x，y)，则它们的联合熵H(X，Y)为

$$
H(X, Y)=\sum_{x \in \Omega} \sum_{y \in \Psi} p(x, y) \log _{2} p(x, y)
$$


