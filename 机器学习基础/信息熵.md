# 1. 熵

对于$P(X) = P(X=x) (x \in R)$，其熵为：

$$
H(X) = -\sum_{x\in R}P(x)log_{2}P(x)
$$

其实它就类似于一个期望的公式，不同的是 ，它衡量的是信息的无序程度。 观察其单调性可以看出来，当一个事件越确定（概率很大，或很小），那么，它的熵就越高。反之，则越低。


例如，一个二元的信息熵可以表示为：


$$
H(X)=-p(x) \log _{2} p(x)-(1-p(x)) \log _{2} p(x)
$$

它的变化曲线如下：

![](https://garden-lu-oss.oss-cn-beijing.aliyuncs.com/images20211019203018.png)

> $H(X) \leq log_2|X|$  , 当且仅当概率平均分布的时候，$H(X)$的最大值为$P(X)=\frac{1}{|X|}$

# 2. 联合熵

如果（X，Y）表示一对离散随机变量的不确定性，即X，Y~p(x，y)，则它们的联合熵H(X，Y)为

$$
H(X, Y)=\sum_{x \in \Omega} \sum_{y \in \Psi} p(x, y) \log _{2} p(x, y)
$$

# 3. 条件熵


在给定随机变量X的情况下，随机变量Y的条件熵定义为：

$$
\begin{aligned}
H(Y \mid X) &=\sum_{x \in \Omega} p(x) H(Y \mid X=x) \\
&=\sum_{x \in \Omega} p(x)\left(-\sum_{y \in \Psi} p(y \mid x) p(x) \log _{2} p(y \mid x)\right) \\
&=-\sum_{x \in \Omega} \sum_{y \in \Psi} p(y \mid x) p(x) \log _{2} p(y \mid x) \\
&=-\sum_{x \in \Omega} \sum_{y \in \Psi} p(x, y) \log _{2} p(y \mid x) \\
H(Y \mid x) &=\sum_{y \in \Psi} p(y \mid x) \log _{2} p(y \mid x)
\end{aligned}
$$



# 4. 自信息

表示时间的不确定性，也可以用来表示事件包含的信息

$$
I(X)=-\log _{2} P(X)
$$


# 5. 互信息

事件






