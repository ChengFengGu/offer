# 1. 决策树

## 1.1. 什么是决策树

从训练数据中找到一组最佳的分类规则，这个树可能有多个，也可能没有。

- 最佳： 损失函数来靠近

找到一个最有特征，对该特征进行最好的划分，并且重复上述步骤，不断丰富内部节点，指导满足指定条件。

## 1.2. 决策树的事实过程


### 1.2.1. 特征选择

特征选择依据的是[信息熵](https://gitee.com/GardenLu/offer/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E4%BF%A1%E6%81%AF%E7%86%B5.md)、信息增益和信息增益比


### 1.2.2. 决策树的生成

> 这里的决策树并不一定是二叉树

#### 1.2.2.1. ID3

- 基于信息增益
- 设定最小信息增益阈值


#### 1.2.2.2. C4.5

- 利用信息增益比。# TODO 更多的改进


### 1.2.3. 决策树的修剪

#### 1.2.3.1. 极小化损失函数

由于树的生成需要考虑到每个特征的每种可能取值，容易出现过拟合，需要对其进行剪枝。

$$
C_\alpha( T)\sum_{t=1}^{\left |T \right |}N_tH_t(T) + \alpha T
$$

其中，T表示确定的一棵决策树，该数共有$|T|$个叶子结点，每个结点下包含训练样本$N_t$个，$H_t(T)$表示训练样本$N_t$的经验熵。

可以看到损失函数包括两部分：决策树分类结果的熵与决策树的叶子结点数量。极小化损失函数时，既需要使得决策树分类后的数据熵很小，同时也需要满足决策树的叶子结点不能太多，即决策树不能太大。其中系数α表示对两者的平衡，α越大，就越倾向于牺牲一定的模型准确率以减少树的叶子结点数量；反之相反。




#### 1.2.3.2. 剪枝的具体步骤


1. 计算每个节点的经验熵 
2. 计算损失函数；

记录某个叶子节点在剪掉之前和之后的损失函数，记为$C_\alpha(T_前)$ 和 $C_\alpha(T_后)$，如果剪掉之后损失变小了，则剪掉这个节点，其父节点成为新的叶子节点。

3. 重复2.

## 1.3. 决策树算法的优缺点

### 1.3.1. 优点

1. 思路简明，计算速度快
2. 规则清晰，可解释性强，可以同时处理数据型特征和其他类型的特征
3. 对缺失值不敏感，可以处理不相关的特征
4. 不需要任何参数假设

### 1.3.2. 缺点

1. 对连续特征变量的处理效果不好
2. 非常容易过拟合


# 2. 随机森林


- 利用多棵决策树对样本进行训练，投票
- 它**可以处理回归问题**和分类问题——CART决策树

## 2.1. CART决策树

- 二叉树

1. 回归树生成

决策树本质上是建立一种划分特征空间的规则，一个特征空间对应着一个输出值。回归树的模型可写为 ：

$$
f(x) = \sum_{k=1}^Kc_kI_{(x\in R_k )}
$$

将特征空间划分为${R_1,R_2,\cdots , R_K}$。其中$c_k$代表空间$R_k$的样本的输出的平均值，$I$为视性函数


当输入空间的划分确定的时候，可以用平方误差$\sum \limits_{x_i\in R_m}(y_i - f(x_i))^2$来表示回归树对与训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。 那么，其实均值就是最有输出值：

$\hat{c} = ave (y_i\mid x_i\in R_m)$



特征选择和树的生成其实就是对特征空间建立划分规则的过程。不同于简单分类决策树，回归树始终满足如下损失函数，并以此为原则进行特征空间的划分。


那么，怎么对空间进行划分呢？ 

假设对于第$j$个变量$x$