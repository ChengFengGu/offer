# 1. 决策树

## 1.1. 什么是决策树

从训练数据中找到一组最佳的分类规则，这个树可能有多个，也可能没有。

- 最佳： 损失函数来靠近

找到一个最有特征，对该特征进行最好的划分，并且重复上述步骤，不断丰富内部节点，指导满足指定条件。


## 1.2. 决策树的事实过程


### 1.2.1. 特征选择

特征选择依据的是[信息熵](https://gitee.com/GardenLu/offer/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E4%BF%A1%E6%81%AF%E7%86%B5.md)、信息增益和信息增益比


### 1.2.2. 决策树的生成


#### 1.2.2.1. ID3

- 基于信息增益
- 设定最小信息增益阈值


#### 1.2.2.2. C4.5

- 利用信息增益比。# TODO 更多的改进


### 1.2.3. 决策树的修剪

#### 1.2.3.1. 极小化损失函数

由于树的生成需要考虑到每个特征的每种可能取值，容易出现过拟合，需要对其进行剪枝。

$$
C_\alpha( T)\sum_{t=1}^{\left |T \right |}N_tH_t(T) + \alpha T
$$

其中，T表示确定的一棵决策树，该数共有$|T|$个叶子结点，每个结点下包含训练样本$N_t$个，$H_t(T)$表示训练样本$N_t$的经验熵。






